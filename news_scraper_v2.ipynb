{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Read in api key for (newsapi-python) newsapi.org and set global variable\n",
    "# #--> pip install newsapi-python\n",
    "#\n",
    "\n",
    "import requests\n",
    "import json\n",
    "from contextlib import closing\n",
    "\n",
    "# get API key for newsapi.org\n",
    "APIKEY_FILE = '../newsapi_key.txt'\n",
    "\n",
    "def read_api_key(api_key_fname):\n",
    "    \"\"\"\n",
    "        read in api key from file. relative path, file just contains api key. return key as string\n",
    "    \"\"\"\n",
    "    with open(api_key_fname) as f:\n",
    "        api_key = f.read()\n",
    "    \n",
    "    return api_key\n",
    "\n",
    "api_key = read_api_key(APIKEY_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initilize NewsApiClient using api key\n",
    "# using python package newsapi-python\n",
    "#--> pip install newsapi-python\n",
    "from newsapi import NewsApiClient\n",
    "\n",
    "# Init\n",
    "newsapi = NewsApiClient(api_key=api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>abc-news</th>\n",
       "      <td>ABC News</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>abc-news-au</th>\n",
       "      <td>ABC News (AU)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aftenposten</th>\n",
       "      <td>Aftenposten</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>al-jazeera-english</th>\n",
       "      <td>Al Jazeera English</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ansa</th>\n",
       "      <td>ANSA.it</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>argaam</th>\n",
       "      <td>Argaam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ars-technica</th>\n",
       "      <td>Ars Technica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ary-news</th>\n",
       "      <td>Ary News</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>associated-press</th>\n",
       "      <td>Associated Press</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>australian-financial-review</th>\n",
       "      <td>Australian Financial Review</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>axios</th>\n",
       "      <td>Axios</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bbc-news</th>\n",
       "      <td>BBC News</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bbc-sport</th>\n",
       "      <td>BBC Sport</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bild</th>\n",
       "      <td>Bild</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>blasting-news-br</th>\n",
       "      <td>Blasting News (BR)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bleacher-report</th>\n",
       "      <td>Bleacher Report</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bloomberg</th>\n",
       "      <td>Bloomberg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>breitbart-news</th>\n",
       "      <td>Breitbart News</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>business-insider</th>\n",
       "      <td>Business Insider</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>business-insider-uk</th>\n",
       "      <td>Business Insider (UK)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>buzzfeed</th>\n",
       "      <td>Buzzfeed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cbc-news</th>\n",
       "      <td>CBC News</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cbs-news</th>\n",
       "      <td>CBS News</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cnbc</th>\n",
       "      <td>CNBC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cnn</th>\n",
       "      <td>CNN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cnn-es</th>\n",
       "      <td>CNN Spanish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>crypto-coins-news</th>\n",
       "      <td>Crypto Coins News</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>daily-mail</th>\n",
       "      <td>Daily Mail</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>der-tagesspiegel</th>\n",
       "      <td>Der Tagesspiegel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>die-zeit</th>\n",
       "      <td>Die Zeit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>techcrunch-cn</th>\n",
       "      <td>TechCrunch (CN)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>techradar</th>\n",
       "      <td>TechRadar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>the-american-conservative</th>\n",
       "      <td>The American Conservative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>the-economist</th>\n",
       "      <td>The Economist</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>the-globe-and-mail</th>\n",
       "      <td>The Globe And Mail</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>the-guardian-au</th>\n",
       "      <td>The Guardian (AU)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>the-guardian-uk</th>\n",
       "      <td>The Guardian (UK)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>the-hill</th>\n",
       "      <td>The Hill</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>the-hindu</th>\n",
       "      <td>The Hindu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>the-huffington-post</th>\n",
       "      <td>The Huffington Post</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>the-irish-times</th>\n",
       "      <td>The Irish Times</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>the-jerusalem-post</th>\n",
       "      <td>The Jerusalem Post</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>the-lad-bible</th>\n",
       "      <td>The Lad Bible</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>the-new-york-times</th>\n",
       "      <td>The New York Times</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>the-next-web</th>\n",
       "      <td>The Next Web</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>the-sport-bible</th>\n",
       "      <td>The Sport Bible</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>the-telegraph</th>\n",
       "      <td>The Telegraph</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>the-times-of-india</th>\n",
       "      <td>The Times of India</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>the-verge</th>\n",
       "      <td>The Verge</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>the-wall-street-journal</th>\n",
       "      <td>The Wall Street Journal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>the-washington-post</th>\n",
       "      <td>The Washington Post</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>the-washington-times</th>\n",
       "      <td>The Washington Times</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time</th>\n",
       "      <td>Time</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>usa-today</th>\n",
       "      <td>USA Today</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vice-news</th>\n",
       "      <td>Vice News</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wired</th>\n",
       "      <td>Wired</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wired-de</th>\n",
       "      <td>Wired.de</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wirtschafts-woche</th>\n",
       "      <td>Wirtschafts Woche</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>xinhua-net</th>\n",
       "      <td>Xinhua Net</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ynet</th>\n",
       "      <td>Ynet</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>138 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    name\n",
       "id                                                      \n",
       "abc-news                                        ABC News\n",
       "abc-news-au                                ABC News (AU)\n",
       "aftenposten                                  Aftenposten\n",
       "al-jazeera-english                    Al Jazeera English\n",
       "ansa                                             ANSA.it\n",
       "argaam                                            Argaam\n",
       "ars-technica                                Ars Technica\n",
       "ary-news                                        Ary News\n",
       "associated-press                        Associated Press\n",
       "australian-financial-review  Australian Financial Review\n",
       "axios                                              Axios\n",
       "bbc-news                                        BBC News\n",
       "bbc-sport                                      BBC Sport\n",
       "bild                                                Bild\n",
       "blasting-news-br                      Blasting News (BR)\n",
       "bleacher-report                          Bleacher Report\n",
       "bloomberg                                      Bloomberg\n",
       "breitbart-news                            Breitbart News\n",
       "business-insider                        Business Insider\n",
       "business-insider-uk                Business Insider (UK)\n",
       "buzzfeed                                        Buzzfeed\n",
       "cbc-news                                        CBC News\n",
       "cbs-news                                        CBS News\n",
       "cnbc                                                CNBC\n",
       "cnn                                                  CNN\n",
       "cnn-es                                       CNN Spanish\n",
       "crypto-coins-news                      Crypto Coins News\n",
       "daily-mail                                    Daily Mail\n",
       "der-tagesspiegel                        Der Tagesspiegel\n",
       "die-zeit                                        Die Zeit\n",
       "...                                                  ...\n",
       "techcrunch-cn                            TechCrunch (CN)\n",
       "techradar                                      TechRadar\n",
       "the-american-conservative      The American Conservative\n",
       "the-economist                              The Economist\n",
       "the-globe-and-mail                    The Globe And Mail\n",
       "the-guardian-au                        The Guardian (AU)\n",
       "the-guardian-uk                        The Guardian (UK)\n",
       "the-hill                                        The Hill\n",
       "the-hindu                                      The Hindu\n",
       "the-huffington-post                  The Huffington Post\n",
       "the-irish-times                          The Irish Times\n",
       "the-jerusalem-post                    The Jerusalem Post\n",
       "the-lad-bible                              The Lad Bible\n",
       "the-new-york-times                    The New York Times\n",
       "the-next-web                                The Next Web\n",
       "the-sport-bible                          The Sport Bible\n",
       "the-telegraph                              The Telegraph\n",
       "the-times-of-india                    The Times of India\n",
       "the-verge                                      The Verge\n",
       "the-wall-street-journal          The Wall Street Journal\n",
       "the-washington-post                  The Washington Post\n",
       "the-washington-times                The Washington Times\n",
       "time                                                Time\n",
       "usa-today                                      USA Today\n",
       "vice-news                                      Vice News\n",
       "wired                                              Wired\n",
       "wired-de                                        Wired.de\n",
       "wirtschafts-woche                      Wirtschafts Woche\n",
       "xinhua-net                                    Xinhua Net\n",
       "ynet                                                Ynet\n",
       "\n",
       "[138 rows x 1 columns]"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Available sources\n",
    "# get sources from NewsApiClient/newsapi.org\n",
    "sources_response = newsapi.get_sources()\n",
    "assert sources_response['status'] == 'ok'\n",
    "\n",
    "# create a name:id python dictonary / map. The ids can be used for requests to NewsApiClient\n",
    "name_id_dict = {s['name']:s['id'] for s in sources_response['sources']}\n",
    "# creat a id:name dictonary\n",
    "id_name_dict = {v: k for k, v in name_id_dict.items()}\n",
    "\n",
    "# create a pandas dataframe from this dictonary with id as first column\n",
    "import pandas as pd\n",
    "\n",
    "id_name_df = pd.DataFrame.from_dict(data=id_name_dict, orient='index', columns=['name'])\n",
    "id_name_df.index.name = 'id'\n",
    "id_name_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "next\n",
      "\n",
      "prev\n",
      "\n",
      "The man in line to replace jailed presidential candidate Luiz Inacio Lula da Silva as the Brazilian Workers' Party standard-bearer on Tuesday denied accusations of corruption.\n",
      "\n",
      "Prosecutors accuse Fernando Haddad of receiving indirect payments to his 2012 campaign for Sao Paulo mayor.\n",
      "\n",
      "Construction company UTC Engenharia got preferential treatment on bids after covering about US$1.6 million of debt associated with Haddad's campaign, according to a former accusation filed Monday. Although Haddad did not request payment directly, he had full control over the scheme, according to prosecutors.\n",
      "\n",
      "On Tuesday, Haddad responded while campaigning in Rio de Janeiro. He said that he cancelled a multimillion dollar project with a company belonging to the UTC group after an employee alerted him that the company was overcharging the government.\n",
      "\n",
      "\"How is it that a mayor who cancels a corrupt construction project gets put through this instead of being thanked for saving the city tens of millions of (Brazilian) reals?\" he said.\n",
      "\n",
      "Haddad is the Workers' Party's candidate for vice president and officials say he'll take the top spot if da Silva is barred from running because of a corruption conviction, as expected.\n",
      "\n",
      "Haddad said the accusations were meant to destabilize the left-leaning party ahead of October's election. Da Silva, serving a 12-year sentence for corruption and money laundering, leads polls with nearly 40 percent of potential voters.\n",
      "\n",
      "Haddad's pull with Brazilian voters, however, is much lower. He failed to get re-elected as mayor in 2016, and polls vary widely on how much support he may garner in the case he takes da Silva's spot atop the ticket.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Article Scraper from a given url\n",
    "\n",
    "requirements: \n",
    "\t-Newspaper3k, https://github.com/codelucas/newspaper, http://newspaper.readthedocs.io/en/latest/, https://newsapi.org/docs/client-libraries/python\n",
    "    --> pip install newspaper3k\n",
    "Notes: \t\n",
    "\tnewspaper offers nlp summary\n",
    "\tarticle.nlp()\n",
    "\tprint(article.summary)\n",
    "\n",
    "\"\"\"\n",
    "# external\n",
    "from newspaper import Article\n",
    "\n",
    "def get_full_article(url):\n",
    "    # does not work for video news sources etc\n",
    "    article = Article(url)\n",
    "    article.download()\n",
    "    article.parse()\n",
    "    text = article.text\n",
    "    if len(text) < 10:\n",
    "        print(\"warning with article url when extracting full text. Function: get_full_article\")\n",
    "        return None\n",
    "    return text\n",
    "\n",
    "# testing\n",
    "test_article_url = \"http://www.foxnews.com/world/2018/08/28/likely-lula-replacement-denies-corruption-charges-in-brazil.html\"\n",
    "print(get_full_article(test_article_url))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get list of article objects from response from newsapi.org\n",
    "#--> pip install newsapi-python\n",
    "#from newsapi import NewsApiClient\n",
    "#-Newspaper3k, https://github.com/codelucas/newspaper, http://newspaper.readthedocs.io/en/latest/, https://newsapi.org/docs/client-libraries/python\n",
    "#--> pip install newspaper3k\n",
    "\n",
    "def get_list_of_article_objects(newsapi_article_response, include_full_articles=True):\n",
    "    \"\"\" given a article response from NewsApiClient return a list of article objects including the source id, title description url and if requested the full text of the article\"\"\"\n",
    "    \n",
    "    # get array of article json objects/list\n",
    "    articles = newsapi_article_response['articles']\n",
    "    \n",
    "    # for article in articles look at 'source', 'title', 'description', 'url'\n",
    "    # extract article sources titles descriptions and links to urls of actual articles\n",
    "    article_objects = []\n",
    "    for a in articles:\n",
    "        \n",
    "        article_object = {\n",
    "            'news_source_id' : a['source']['id'], # article news source id\n",
    "            'title' : a['title'], # article title \n",
    "            'description' : a['description'], # breif article description\n",
    "            'url' : a['url'], # full article url\n",
    "        }\n",
    "        \n",
    "        if include_full_articles:\n",
    "            article_object['text'] = get_full_article(a['url']) # get full article text using Newspaper3k -- often None\n",
    "        \n",
    "        if None not in article_object.values():\n",
    "            article_objects.append(article_object)\n",
    "        \n",
    "    return article_objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# or by using python package\n",
    "# -- https://github.com/mattlisiv/newsapi-python\n",
    "#--> pip install newsapi-python\n",
    "#from newsapi import NewsApiClient\n",
    "\n",
    "def get_all_articles(source_id, api_key, max_articles=500, include_full_article_text=True):\n",
    "    \n",
    "    # init\n",
    "    newsapi = NewsApiClient(api_key=api_key)\n",
    "    \n",
    "    print(source_id)\n",
    "    \n",
    "    all_article_objects = []\n",
    "    page_number = 1\n",
    "    \n",
    "    while len(all_article_objects) < max_articles:\n",
    "        page_number = page_number + 1\n",
    "        response = newsapi.get_everything(sources=source_id,\n",
    "                                      language='en',\n",
    "                                      page=page_number, # can also use dates\n",
    "                                        page_size=100) # 100 is maximum page size\n",
    "    \n",
    "        assert response['status'] == 'ok'\n",
    "        \n",
    "        all_article_objects += get_list_of_article_objects(response, include_full_articles=include_full_article_text)\n",
    "        print(\"number of articles collected: %s\" % len(all_article_objects[:max_articles]))\n",
    "        \n",
    "    return all_article_objects[:max_articles]\n",
    "        \n",
    "#\n",
    "# Testing\n",
    "#\n",
    "\n",
    "# source\n",
    "#selected_source_id = name_id_dict['Fox News']\n",
    "    \n",
    "# number of documents\n",
    "#num_documents = 50\n",
    "\n",
    "# example -- set include full article text to True for scrapping the actual site\n",
    "#all_article_objects = get_all_articles(selected_source_id, api_key, max_articles=num_documents, include_full_article_text=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn list of article objects into single giant corpus\n",
    "\n",
    "def article_objects_to_single_document_strings(article_objects):\n",
    "    \"\"\"combines title, description and text of all articles into single string\"\"\"\n",
    "    \n",
    "    concatinated_article_objects = []\n",
    "    for ao in article_objects:\n",
    "        article_string = ' '\n",
    "        title = ao['title']\n",
    "        description = ao['description']\n",
    "        text = ''\n",
    "        if 'text' in ao:\n",
    "            text = ao['text']\n",
    "            if text == None:\n",
    "                text = ''\n",
    "                \n",
    "        article_string += title + '. ' + description + '. ' + text + '. '\n",
    "        concatinated_article_object = {\n",
    "            'id' : ao['news_source_id'],\n",
    "            'article_string' : article_string\n",
    "        }\n",
    "        concatinated_article_objects.append(concatinated_article_object)\n",
    "    \n",
    "    return concatinated_article_objects\n",
    "        \n",
    "#\n",
    "# Testing\n",
    "#\n",
    "\n",
    "# source\n",
    "#selected_source_id = name_id_dict['Fox News']\n",
    "    \n",
    "# number of documents\n",
    "#num_documents = 50\n",
    "\n",
    "# example -- set include full article text to True for scrapping the actual site\n",
    "#all_article_objects = get_all_articles(selected_source_id, api_key, max_articles=num_documents, include_full_article_text=True)\n",
    "\n",
    "#simple_article_objects = article_objects_to_single_document_strings(all_article_objects)\n",
    "#simple_article_objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>bias</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>breitbart-news</th>\n",
       "      <td>Breitbart News</td>\n",
       "      <td>conservative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cnn</th>\n",
       "      <td>CNN</td>\n",
       "      <td>liberal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fox-news</th>\n",
       "      <td>Fox News</td>\n",
       "      <td>conservative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>msnbc</th>\n",
       "      <td>MSNBC</td>\n",
       "      <td>liberal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>reuters</th>\n",
       "      <td>Reuters</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          name          bias\n",
       "id                                          \n",
       "breitbart-news  Breitbart News  conservative\n",
       "cnn                        CNN       liberal\n",
       "fox-news              Fox News  conservative\n",
       "msnbc                    MSNBC       liberal\n",
       "reuters                Reuters       neutral"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#\n",
    "# Selected sources and bias labels (conservative, nuetral, liberal)\n",
    "# \n",
    "# Selected sources 'Breitbart News', 'Fox News', 'Reuters', 'MSNBC', and 'CNN' - (conservative, conservative, neutral, liberal, liberal)\n",
    "#\n",
    "selected_source_ids = ['cnn', 'msnbc', 'reuters', 'fox-news', 'breitbart-news']\n",
    "\n",
    "# bias labels python dictonary\n",
    "id_bias_dict = {\n",
    "    'cnn' : 'liberal',\n",
    "    'msnbc' : 'liberal',\n",
    "    'reuters' : 'neutral',\n",
    "    'fox-news' : 'conservative',\n",
    "    'breitbart-news' : 'conservative'\n",
    "}\n",
    "\n",
    "id_bias_dict\n",
    "\n",
    "# pandas data frame version\n",
    "id_bias_df = pd.DataFrame.from_dict(data=id_bias_dict, orient='index', columns=['bias'])\n",
    "id_bias_df.index.name = 'id'\n",
    "id_bias_df\n",
    "\n",
    "# join name id dataframe and bias id dataframe\n",
    "selected_news_sources_df = pd.merge(id_name_df, id_bias_df, on='id')\n",
    "selected_news_sources_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "breitbart-news\n",
      "number of articles collected: 100\n",
      "number of articles collected: 200\n",
      "cnn\n",
      "number of articles collected: 98\n",
      "number of articles collected: 198\n",
      "number of articles collected: 200\n",
      "fox-news\n",
      "number of articles collected: 100\n",
      "number of articles collected: 200\n",
      "msnbc\n",
      "number of articles collected: 100\n",
      "number of articles collected: 200\n",
      "reuters\n",
      "number of articles collected: 100\n",
      "number of articles collected: 200\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>frequency</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>term</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>mexican</th>\n",
       "      <td>1403</td>\n",
       "      <td>breitbart-news</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cartel</th>\n",
       "      <td>368</td>\n",
       "      <td>breitbart-news</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>boss</th>\n",
       "      <td>291</td>\n",
       "      <td>breitbart-news</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>offers</th>\n",
       "      <td>1535</td>\n",
       "      <td>breitbart-news</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100k</th>\n",
       "      <td>4</td>\n",
       "      <td>breitbart-news</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>assassination</th>\n",
       "      <td>180</td>\n",
       "      <td>breitbart-news</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>breitbart</th>\n",
       "      <td>302</td>\n",
       "      <td>breitbart-news</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>texas</th>\n",
       "      <td>2206</td>\n",
       "      <td>breitbart-news</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>writer</th>\n",
       "      <td>2436</td>\n",
       "      <td>breitbart-news</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>arrested</th>\n",
       "      <td>169</td>\n",
       "      <td>breitbart-news</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>days</th>\n",
       "      <td>585</td>\n",
       "      <td>breitbart-news</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>29</td>\n",
       "      <td>breitbart-news</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hours</th>\n",
       "      <td>1052</td>\n",
       "      <td>breitbart-news</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mexico</th>\n",
       "      <td>1404</td>\n",
       "      <td>breitbart-news</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>placed</th>\n",
       "      <td>1639</td>\n",
       "      <td>breitbart-news</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hit</th>\n",
       "      <td>1035</td>\n",
       "      <td>breitbart-news</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>contributing</th>\n",
       "      <td>524</td>\n",
       "      <td>breitbart-news</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>writers</th>\n",
       "      <td>2437</td>\n",
       "      <td>breitbart-news</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chronicles</th>\n",
       "      <td>425</td>\n",
       "      <td>breitbart-news</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>project</th>\n",
       "      <td>1730</td>\n",
       "      <td>breitbart-news</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>military</th>\n",
       "      <td>1417</td>\n",
       "      <td>breitbart-news</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>forces</th>\n",
       "      <td>883</td>\n",
       "      <td>breitbart-news</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>captured</th>\n",
       "      <td>357</td>\n",
       "      <td>breitbart-news</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>donald</th>\n",
       "      <td>691</td>\n",
       "      <td>breitbart-news</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>trump</th>\n",
       "      <td>2272</td>\n",
       "      <td>breitbart-news</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>says</th>\n",
       "      <td>1939</td>\n",
       "      <td>breitbart-news</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>facebook</th>\n",
       "      <td>814</td>\n",
       "      <td>breitbart-news</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>twitter</th>\n",
       "      <td>2284</td>\n",
       "      <td>breitbart-news</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bias</th>\n",
       "      <td>263</td>\n",
       "      <td>breitbart-news</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>personally</th>\n",
       "      <td>1626</td>\n",
       "      <td>breitbart-news</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>touching</th>\n",
       "      <td>2035</td>\n",
       "      <td>reuters</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>developments</th>\n",
       "      <td>547</td>\n",
       "      <td>reuters</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>negotiations</th>\n",
       "      <td>1313</td>\n",
       "      <td>reuters</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>underpinned</th>\n",
       "      <td>2091</td>\n",
       "      <td>reuters</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sentiment</th>\n",
       "      <td>1773</td>\n",
       "      <td>reuters</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wasted</th>\n",
       "      <td>2157</td>\n",
       "      <td>reuters</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>elekta</th>\n",
       "      <td>639</td>\n",
       "      <td>reuters</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stands</th>\n",
       "      <td>1884</td>\n",
       "      <td>reuters</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>outlook</th>\n",
       "      <td>1385</td>\n",
       "      <td>reuters</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>q1</th>\n",
       "      <td>1562</td>\n",
       "      <td>reuters</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018</th>\n",
       "      <td>20</td>\n",
       "      <td>reuters</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>driver</th>\n",
       "      <td>605</td>\n",
       "      <td>reuters</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unity</th>\n",
       "      <td>2102</td>\n",
       "      <td>reuters</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>adds</th>\n",
       "      <td>67</td>\n",
       "      <td>reuters</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>background</th>\n",
       "      <td>195</td>\n",
       "      <td>reuters</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>optimistic</th>\n",
       "      <td>1373</td>\n",
       "      <td>reuters</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>negotiating</th>\n",
       "      <td>1312</td>\n",
       "      <td>reuters</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>probably</th>\n",
       "      <td>1517</td>\n",
       "      <td>reuters</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accord</th>\n",
       "      <td>47</td>\n",
       "      <td>reuters</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rough</th>\n",
       "      <td>1707</td>\n",
       "      <td>reuters</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>reporter</th>\n",
       "      <td>1645</td>\n",
       "      <td>reuters</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>narration</th>\n",
       "      <td>1293</td>\n",
       "      <td>reuters</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>factors</th>\n",
       "      <td>719</td>\n",
       "      <td>reuters</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ftse</th>\n",
       "      <td>813</td>\n",
       "      <td>reuters</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>2</td>\n",
       "      <td>reuters</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>3</td>\n",
       "      <td>reuters</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>points</th>\n",
       "      <td>1467</td>\n",
       "      <td>reuters</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>553</th>\n",
       "      <td>31</td>\n",
       "      <td>reuters</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bookmakers</th>\n",
       "      <td>259</td>\n",
       "      <td>reuters</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cash</th>\n",
       "      <td>331</td>\n",
       "      <td>reuters</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10894 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               frequency              id\n",
       "term                                    \n",
       "mexican             1403  breitbart-news\n",
       "cartel               368  breitbart-news\n",
       "boss                 291  breitbart-news\n",
       "offers              1535  breitbart-news\n",
       "100k                   4  breitbart-news\n",
       "assassination        180  breitbart-news\n",
       "breitbart            302  breitbart-news\n",
       "texas               2206  breitbart-news\n",
       "writer              2436  breitbart-news\n",
       "arrested             169  breitbart-news\n",
       "days                 585  breitbart-news\n",
       "48                    29  breitbart-news\n",
       "hours               1052  breitbart-news\n",
       "mexico              1404  breitbart-news\n",
       "placed              1639  breitbart-news\n",
       "hit                 1035  breitbart-news\n",
       "contributing         524  breitbart-news\n",
       "writers             2437  breitbart-news\n",
       "chronicles           425  breitbart-news\n",
       "project             1730  breitbart-news\n",
       "military            1417  breitbart-news\n",
       "forces               883  breitbart-news\n",
       "captured             357  breitbart-news\n",
       "donald               691  breitbart-news\n",
       "trump               2272  breitbart-news\n",
       "says                1939  breitbart-news\n",
       "facebook             814  breitbart-news\n",
       "twitter             2284  breitbart-news\n",
       "bias                 263  breitbart-news\n",
       "personally          1626  breitbart-news\n",
       "...                  ...             ...\n",
       "touching            2035         reuters\n",
       "developments         547         reuters\n",
       "negotiations        1313         reuters\n",
       "underpinned         2091         reuters\n",
       "sentiment           1773         reuters\n",
       "wasted              2157         reuters\n",
       "elekta               639         reuters\n",
       "stands              1884         reuters\n",
       "outlook             1385         reuters\n",
       "q1                  1562         reuters\n",
       "2018                  20         reuters\n",
       "driver               605         reuters\n",
       "unity               2102         reuters\n",
       "adds                  67         reuters\n",
       "background           195         reuters\n",
       "optimistic          1373         reuters\n",
       "negotiating         1312         reuters\n",
       "probably            1517         reuters\n",
       "accord                47         reuters\n",
       "rough               1707         reuters\n",
       "reporter            1645         reuters\n",
       "narration           1293         reuters\n",
       "factors              719         reuters\n",
       "ftse                 813         reuters\n",
       "100                    2         reuters\n",
       "11                     3         reuters\n",
       "points              1467         reuters\n",
       "553                   31         reuters\n",
       "bookmakers           259         reuters\n",
       "cash                 331         reuters\n",
       "\n",
       "[10894 rows x 2 columns]"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#\n",
    "# scrape from selected sources\n",
    "#\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "\n",
    "# sources\n",
    "selected_source_ids = list(selected_news_sources_df.index.values)\n",
    "    \n",
    "# number of documents\n",
    "num_documents_per_source = 200\n",
    "\n",
    "def scrape_from_sources_bag_of_words(api_key, selected_source_ids, num_documents_per_source):\n",
    "    \n",
    " \n",
    "    frames = []\n",
    "   \n",
    "    for sid in selected_source_ids:\n",
    "        \n",
    "        #set include full article text to True for scrapping the actual site\n",
    "        all_article_objects = get_all_articles(sid, api_key, max_articles=num_documents_per_source, include_full_article_text=False)\n",
    "\n",
    "        simple_article_objects = article_objects_to_single_document_strings(all_article_objects)\n",
    "        source_articles = [x['article_string'] for x in simple_article_objects]\n",
    "\n",
    "        #count_vect = CountVectorizer()\n",
    "        #bag_words = count_vect.fit_transform(source_articles)\n",
    "        count_vect = CountVectorizer(input=source_articles, stop_words='english')\n",
    "        count_vect.fit_transform(source_articles)\n",
    "        \n",
    "        \n",
    "        vocab_df = pd.DataFrame.from_dict(data=count_vect.vocabulary_, orient='index', columns=['frequency'])\n",
    "        vocab_df.index.name = 'term'\n",
    "        vocab_df['id'] = sid\n",
    "        \n",
    "        # reset\n",
    "        #%reset_selective -f \"^count_vect$\"\n",
    "        #del count_vect\n",
    "        #print(count_vect.vocabulary_)\n",
    "        \n",
    "        frames.append(vocab_df)\n",
    "    \n",
    "    source_id_vocab_df = pd.concat(frames)\n",
    "    \n",
    "    return source_id_vocab_df\n",
    "    \n",
    "\n",
    "# scrape from all selected sources and store as term frequency id in panda dataframe\n",
    "sources_bag_of_words_df = scrape_from_sources_bag_of_words(api_key, selected_source_ids, num_documents_per_source)\n",
    "\n",
    "sources_bag_of_words_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 20 words in breitbart-news source.\n",
      "          frequency              id\n",
      "term                               \n",
      "worry          2429  breitbart-news\n",
      "worse          2430  breitbart-news\n",
      "worsened       2431  breitbart-news\n",
      "worst          2432  breitbart-news\n",
      "worth          2433  breitbart-news\n",
      "wouldn         2434  breitbart-news\n",
      "wowing         2435  breitbart-news\n",
      "writer         2436  breitbart-news\n",
      "writers        2437  breitbart-news\n",
      "wrong          2438  breitbart-news\n",
      "wrote          2439  breitbart-news\n",
      "year           2440  breitbart-news\n",
      "years          2441  breitbart-news\n",
      "yelling        2442  breitbart-news\n",
      "york           2443  breitbart-news\n",
      "yorker         2444  breitbart-news\n",
      "yorkers        2445  breitbart-news\n",
      "yuma           2446  breitbart-news\n",
      "zombie         2447  breitbart-news\n",
      "zucker         2448  breitbart-news\n",
      "Top 20 words in cnn source.\n",
      "           frequency   id\n",
      "term                     \n",
      "worldwide       2162  cnn\n",
      "worried         2163  cnn\n",
      "worth           2164  cnn\n",
      "wows            2165  cnn\n",
      "writ            2166  cnn\n",
      "writes          2167  cnn\n",
      "written         2168  cnn\n",
      "wrong           2169  cnn\n",
      "wrongful        2170  cnn\n",
      "wrote           2171  cnn\n",
      "wuerl           2172  cnn\n",
      "year            2173  cnn\n",
      "years           2174  cnn\n",
      "yes             2175  cnn\n",
      "yo              2176  cnn\n",
      "york            2177  cnn\n",
      "young           2178  cnn\n",
      "yulin           2179  cnn\n",
      "zaghari         2180  cnn\n",
      "zealand         2181  cnn\n",
      "Top 20 words in fox-news source.\n",
      "           frequency        id\n",
      "term                          \n",
      "words           2208  fox-news\n",
      "work            2209  fox-news\n",
      "workers         2210  fox-news\n",
      "workplace       2211  fox-news\n",
      "works           2212  fox-news\n",
      "world           2213  fox-news\n",
      "worried         2214  fox-news\n",
      "worse           2215  fox-news\n",
      "worst           2216  fox-news\n",
      "wouldn          2217  fox-news\n",
      "wounded         2218  fox-news\n",
      "writer          2219  fox-news\n",
      "wrong           2220  fox-news\n",
      "year            2221  fox-news\n",
      "years           2222  fox-news\n",
      "york            2223  fox-news\n",
      "young           2224  fox-news\n",
      "youth           2225  fox-news\n",
      "zero            2226  fox-news\n",
      "zimbabwe        2227  fox-news\n",
      "Top 20 words in msnbc source.\n",
      "           frequency     id\n",
      "term                       \n",
      "wore            1798  msnbc\n",
      "work            1799  msnbc\n",
      "worked          1800  msnbc\n",
      "working         1801  msnbc\n",
      "world           1802  msnbc\n",
      "worried         1803  msnbc\n",
      "worrying        1804  msnbc\n",
      "worthy          1805  msnbc\n",
      "wrapped         1806  msnbc\n",
      "writer          1807  msnbc\n",
      "writes          1808  msnbc\n",
      "wrong           1809  msnbc\n",
      "wrote           1810  msnbc\n",
      "wsj             1811  msnbc\n",
      "yale            1812  msnbc\n",
      "year            1813  msnbc\n",
      "years           1814  msnbc\n",
      "yesterday       1815  msnbc\n",
      "yevgeniy        1816  msnbc\n",
      "york            1817  msnbc\n",
      "Top 20 words in reuters source.\n",
      "            frequency       id\n",
      "term                          \n",
      "worries          2197  reuters\n",
      "worst            2198  reuters\n",
      "wounded          2199  reuters\n",
      "wpp              2200  reuters\n",
      "wsj              2201  reuters\n",
      "wuhu             2202  reuters\n",
      "www              2203  reuters\n",
      "xenophobic       2204  reuters\n",
      "xinjiang         2205  reuters\n",
      "yanked           2206  reuters\n",
      "yardie           2207  reuters\n",
      "year             2208  reuters\n",
      "years            2209  reuters\n",
      "yields           2210  reuters\n",
      "york             2211  reuters\n",
      "young            2212  reuters\n",
      "younger          2213  reuters\n",
      "yuan             2214  reuters\n",
      "zte              2215  reuters\n",
      "zucker           2216  reuters\n"
     ]
    }
   ],
   "source": [
    "# print out n most common words by source\n",
    "n = 20\n",
    "df_grouped = sources_bag_of_words_df.groupby(by='id')\n",
    "for sid,grp in df_grouped:\n",
    "\n",
    "    top_words = grp.sort_values(by='frequency')[-n:]\n",
    "    print('Top %s words in %s source.' % (n, sid))\n",
    "    print(top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
