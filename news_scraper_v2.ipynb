{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Read in api key for (newsapi-python) newsapi.org and set global variable\n",
    "# #--> pip install newsapi-python\n",
    "#\n",
    "\n",
    "import requests\n",
    "import json\n",
    "from contextlib import closing\n",
    "\n",
    "# get API key for newsapi.org\n",
    "APIKEY_FILE = '../newsapi_key.txt'\n",
    "\n",
    "def read_api_key(api_key_fname):\n",
    "    \"\"\"\n",
    "        read in api key from file. relative path, file just contains api key. return key as string\n",
    "    \"\"\"\n",
    "    with open(api_key_fname) as f:\n",
    "        api_key = f.read()\n",
    "    \n",
    "    return api_key\n",
    "\n",
    "api_key = read_api_key(APIKEY_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initilize NewsApiClient using api key\n",
    "# using python package newsapi-python\n",
    "#--> pip install newsapi-python\n",
    "from newsapi import NewsApiClient\n",
    "\n",
    "# Init\n",
    "newsapi = NewsApiClient(api_key=api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>abc-news</th>\n",
       "      <td>ABC News</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>abc-news-au</th>\n",
       "      <td>ABC News (AU)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aftenposten</th>\n",
       "      <td>Aftenposten</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>al-jazeera-english</th>\n",
       "      <td>Al Jazeera English</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ansa</th>\n",
       "      <td>ANSA.it</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>argaam</th>\n",
       "      <td>Argaam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ars-technica</th>\n",
       "      <td>Ars Technica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ary-news</th>\n",
       "      <td>Ary News</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>associated-press</th>\n",
       "      <td>Associated Press</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>australian-financial-review</th>\n",
       "      <td>Australian Financial Review</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>axios</th>\n",
       "      <td>Axios</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bbc-news</th>\n",
       "      <td>BBC News</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bbc-sport</th>\n",
       "      <td>BBC Sport</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bild</th>\n",
       "      <td>Bild</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>blasting-news-br</th>\n",
       "      <td>Blasting News (BR)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bleacher-report</th>\n",
       "      <td>Bleacher Report</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bloomberg</th>\n",
       "      <td>Bloomberg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>breitbart-news</th>\n",
       "      <td>Breitbart News</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>business-insider</th>\n",
       "      <td>Business Insider</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>business-insider-uk</th>\n",
       "      <td>Business Insider (UK)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>buzzfeed</th>\n",
       "      <td>Buzzfeed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cbc-news</th>\n",
       "      <td>CBC News</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cbs-news</th>\n",
       "      <td>CBS News</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cnbc</th>\n",
       "      <td>CNBC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cnn</th>\n",
       "      <td>CNN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cnn-es</th>\n",
       "      <td>CNN Spanish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>crypto-coins-news</th>\n",
       "      <td>Crypto Coins News</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>daily-mail</th>\n",
       "      <td>Daily Mail</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>der-tagesspiegel</th>\n",
       "      <td>Der Tagesspiegel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>die-zeit</th>\n",
       "      <td>Die Zeit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>techcrunch-cn</th>\n",
       "      <td>TechCrunch (CN)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>techradar</th>\n",
       "      <td>TechRadar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>the-american-conservative</th>\n",
       "      <td>The American Conservative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>the-economist</th>\n",
       "      <td>The Economist</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>the-globe-and-mail</th>\n",
       "      <td>The Globe And Mail</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>the-guardian-au</th>\n",
       "      <td>The Guardian (AU)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>the-guardian-uk</th>\n",
       "      <td>The Guardian (UK)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>the-hill</th>\n",
       "      <td>The Hill</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>the-hindu</th>\n",
       "      <td>The Hindu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>the-huffington-post</th>\n",
       "      <td>The Huffington Post</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>the-irish-times</th>\n",
       "      <td>The Irish Times</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>the-jerusalem-post</th>\n",
       "      <td>The Jerusalem Post</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>the-lad-bible</th>\n",
       "      <td>The Lad Bible</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>the-new-york-times</th>\n",
       "      <td>The New York Times</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>the-next-web</th>\n",
       "      <td>The Next Web</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>the-sport-bible</th>\n",
       "      <td>The Sport Bible</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>the-telegraph</th>\n",
       "      <td>The Telegraph</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>the-times-of-india</th>\n",
       "      <td>The Times of India</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>the-verge</th>\n",
       "      <td>The Verge</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>the-wall-street-journal</th>\n",
       "      <td>The Wall Street Journal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>the-washington-post</th>\n",
       "      <td>The Washington Post</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>the-washington-times</th>\n",
       "      <td>The Washington Times</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time</th>\n",
       "      <td>Time</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>usa-today</th>\n",
       "      <td>USA Today</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vice-news</th>\n",
       "      <td>Vice News</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wired</th>\n",
       "      <td>Wired</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wired-de</th>\n",
       "      <td>Wired.de</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wirtschafts-woche</th>\n",
       "      <td>Wirtschafts Woche</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>xinhua-net</th>\n",
       "      <td>Xinhua Net</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ynet</th>\n",
       "      <td>Ynet</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>138 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    name\n",
       "id                                                      \n",
       "abc-news                                        ABC News\n",
       "abc-news-au                                ABC News (AU)\n",
       "aftenposten                                  Aftenposten\n",
       "al-jazeera-english                    Al Jazeera English\n",
       "ansa                                             ANSA.it\n",
       "argaam                                            Argaam\n",
       "ars-technica                                Ars Technica\n",
       "ary-news                                        Ary News\n",
       "associated-press                        Associated Press\n",
       "australian-financial-review  Australian Financial Review\n",
       "axios                                              Axios\n",
       "bbc-news                                        BBC News\n",
       "bbc-sport                                      BBC Sport\n",
       "bild                                                Bild\n",
       "blasting-news-br                      Blasting News (BR)\n",
       "bleacher-report                          Bleacher Report\n",
       "bloomberg                                      Bloomberg\n",
       "breitbart-news                            Breitbart News\n",
       "business-insider                        Business Insider\n",
       "business-insider-uk                Business Insider (UK)\n",
       "buzzfeed                                        Buzzfeed\n",
       "cbc-news                                        CBC News\n",
       "cbs-news                                        CBS News\n",
       "cnbc                                                CNBC\n",
       "cnn                                                  CNN\n",
       "cnn-es                                       CNN Spanish\n",
       "crypto-coins-news                      Crypto Coins News\n",
       "daily-mail                                    Daily Mail\n",
       "der-tagesspiegel                        Der Tagesspiegel\n",
       "die-zeit                                        Die Zeit\n",
       "...                                                  ...\n",
       "techcrunch-cn                            TechCrunch (CN)\n",
       "techradar                                      TechRadar\n",
       "the-american-conservative      The American Conservative\n",
       "the-economist                              The Economist\n",
       "the-globe-and-mail                    The Globe And Mail\n",
       "the-guardian-au                        The Guardian (AU)\n",
       "the-guardian-uk                        The Guardian (UK)\n",
       "the-hill                                        The Hill\n",
       "the-hindu                                      The Hindu\n",
       "the-huffington-post                  The Huffington Post\n",
       "the-irish-times                          The Irish Times\n",
       "the-jerusalem-post                    The Jerusalem Post\n",
       "the-lad-bible                              The Lad Bible\n",
       "the-new-york-times                    The New York Times\n",
       "the-next-web                                The Next Web\n",
       "the-sport-bible                          The Sport Bible\n",
       "the-telegraph                              The Telegraph\n",
       "the-times-of-india                    The Times of India\n",
       "the-verge                                      The Verge\n",
       "the-wall-street-journal          The Wall Street Journal\n",
       "the-washington-post                  The Washington Post\n",
       "the-washington-times                The Washington Times\n",
       "time                                                Time\n",
       "usa-today                                      USA Today\n",
       "vice-news                                      Vice News\n",
       "wired                                              Wired\n",
       "wired-de                                        Wired.de\n",
       "wirtschafts-woche                      Wirtschafts Woche\n",
       "xinhua-net                                    Xinhua Net\n",
       "ynet                                                Ynet\n",
       "\n",
       "[138 rows x 1 columns]"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Available sources\n",
    "# get sources from NewsApiClient/newsapi.org\n",
    "sources_response = newsapi.get_sources()\n",
    "assert sources_response['status'] == 'ok'\n",
    "\n",
    "# create a name:id python dictonary / map. The ids can be used for requests to NewsApiClient\n",
    "name_id_dict = {s['name']:s['id'] for s in sources_response['sources']}\n",
    "# creat a id:name dictonary\n",
    "id_name_dict = {v: k for k, v in name_id_dict.items()}\n",
    "\n",
    "# create a pandas dataframe from this dictonary with id as first column\n",
    "import pandas as pd\n",
    "\n",
    "id_name_df = pd.DataFrame.from_dict(data=id_name_dict, orient='index', columns=['name'])\n",
    "id_name_df.index.name = 'id'\n",
    "id_name_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "next\n",
      "\n",
      "prev\n",
      "\n",
      "The man in line to replace jailed presidential candidate Luiz Inacio Lula da Silva as the Brazilian Workers' Party standard-bearer on Tuesday denied accusations of corruption.\n",
      "\n",
      "Prosecutors accuse Fernando Haddad of receiving indirect payments to his 2012 campaign for Sao Paulo mayor.\n",
      "\n",
      "Construction company UTC Engenharia got preferential treatment on bids after covering about US$1.6 million of debt associated with Haddad's campaign, according to a former accusation filed Monday. Although Haddad did not request payment directly, he had full control over the scheme, according to prosecutors.\n",
      "\n",
      "On Tuesday, Haddad responded while campaigning in Rio de Janeiro. He said that he cancelled a multimillion dollar project with a company belonging to the UTC group after an employee alerted him that the company was overcharging the government.\n",
      "\n",
      "\"How is it that a mayor who cancels a corrupt construction project gets put through this instead of being thanked for saving the city tens of millions of (Brazilian) reals?\" he said.\n",
      "\n",
      "Haddad is the Workers' Party's candidate for vice president and officials say he'll take the top spot if da Silva is barred from running because of a corruption conviction, as expected.\n",
      "\n",
      "Haddad said the accusations were meant to destabilize the left-leaning party ahead of October's election. Da Silva, serving a 12-year sentence for corruption and money laundering, leads polls with nearly 40 percent of potential voters.\n",
      "\n",
      "Haddad's pull with Brazilian voters, however, is much lower. He failed to get re-elected as mayor in 2016, and polls vary widely on how much support he may garner in the case he takes da Silva's spot atop the ticket.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Article Scraper from a given url\n",
    "\n",
    "requirements: \n",
    "\t-Newspaper3k, https://github.com/codelucas/newspaper, http://newspaper.readthedocs.io/en/latest/, https://newsapi.org/docs/client-libraries/python\n",
    "    --> pip install newspaper3k\n",
    "Notes: \t\n",
    "\tnewspaper offers nlp summary\n",
    "\tarticle.nlp()\n",
    "\tprint(article.summary)\n",
    "\n",
    "\"\"\"\n",
    "# external\n",
    "from newspaper import Article\n",
    "\n",
    "def get_full_article(url):\n",
    "    # does not work for video news sources etc\n",
    "    article = Article(url)\n",
    "    article.download()\n",
    "    article.parse()\n",
    "    text = article.text\n",
    "    if len(text) < 10:\n",
    "        print(\"warning with article url when extracting full text. Function: get_full_article\")\n",
    "        return None\n",
    "    return text\n",
    "\n",
    "# testing\n",
    "test_article_url = \"http://www.foxnews.com/world/2018/08/28/likely-lula-replacement-denies-corruption-charges-in-brazil.html\"\n",
    "print(get_full_article(test_article_url))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get list of article objects from response from newsapi.org\n",
    "#--> pip install newsapi-python\n",
    "#from newsapi import NewsApiClient\n",
    "#-Newspaper3k, https://github.com/codelucas/newspaper, http://newspaper.readthedocs.io/en/latest/, https://newsapi.org/docs/client-libraries/python\n",
    "#--> pip install newspaper3k\n",
    "\n",
    "def get_list_of_article_objects(newsapi_article_response, include_full_articles=True):\n",
    "    \"\"\" given a article response from NewsApiClient return a list of article objects including the source id, title description url and if requested the full text of the article\"\"\"\n",
    "    \n",
    "    # get array of article json objects/list\n",
    "    articles = newsapi_article_response['articles']\n",
    "    \n",
    "    # for article in articles look at 'source', 'title', 'description', 'url'\n",
    "    # extract article sources titles descriptions and links to urls of actual articles\n",
    "    article_objects = []\n",
    "    for a in articles:\n",
    "        \n",
    "        article_object = {\n",
    "            'news_source_id' : a['source']['id'], # article news source id\n",
    "            'title' : a['title'], # article title \n",
    "            'description' : a['description'], # breif article description\n",
    "            'url' : a['url'], # full article url\n",
    "        }\n",
    "        \n",
    "        if include_full_articles:\n",
    "            article_object['text'] = get_full_article(a['url']) # get full article text using Newspaper3k -- often None\n",
    "        \n",
    "        if None not in article_object.values():\n",
    "            article_objects.append(article_object)\n",
    "        \n",
    "    return article_objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# or by using python package\n",
    "# -- https://github.com/mattlisiv/newsapi-python\n",
    "#--> pip install newsapi-python\n",
    "#from newsapi import NewsApiClient\n",
    "\n",
    "def get_all_articles(source_id, api_key, max_articles=500, include_full_article_text=True):\n",
    "    \n",
    "    # init\n",
    "    newsapi = NewsApiClient(api_key=api_key)\n",
    "    \n",
    "    print(source_id)\n",
    "    \n",
    "    all_article_objects = []\n",
    "    page_number = 1\n",
    "    \n",
    "    while len(all_article_objects) < max_articles:\n",
    "        page_number = page_number + 1\n",
    "        response = newsapi.get_everything(sources=source_id,\n",
    "                                      language='en',\n",
    "                                      page=page_number, # can also use dates\n",
    "                                        page_size=100) # 100 is maximum page size\n",
    "    \n",
    "        assert response['status'] == 'ok'\n",
    "        \n",
    "        all_article_objects += get_list_of_article_objects(response, include_full_articles=include_full_article_text)\n",
    "        print(\"number of articles collected: %s\" % len(all_article_objects[:max_articles]))\n",
    "        \n",
    "    return all_article_objects[:max_articles]\n",
    "        \n",
    "#\n",
    "# Testing\n",
    "#\n",
    "\n",
    "# source\n",
    "#selected_source_id = name_id_dict['Fox News']\n",
    "    \n",
    "# number of documents\n",
    "#num_documents = 50\n",
    "\n",
    "# example -- set include full article text to True for scrapping the actual site\n",
    "#all_article_objects = get_all_articles(selected_source_id, api_key, max_articles=num_documents, include_full_article_text=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn list of article objects into single giant corpus\n",
    "\n",
    "def article_objects_to_single_document_strings(article_objects):\n",
    "    \"\"\"combines title, description and text of all articles into single string\"\"\"\n",
    "    \n",
    "    concatinated_article_objects = []\n",
    "    for ao in article_objects:\n",
    "        article_string = ' '\n",
    "        title = ao['title']\n",
    "        description = ao['description']\n",
    "        text = ''\n",
    "        if 'text' in ao:\n",
    "            text = ao['text']\n",
    "            if text == None:\n",
    "                text = ''\n",
    "                \n",
    "        article_string += title + '. ' + description + '. ' + text + '. '\n",
    "        concatinated_article_object = {\n",
    "            'id' : ao['news_source_id'],\n",
    "            'article_string' : article_string\n",
    "        }\n",
    "        concatinated_article_objects.append(concatinated_article_object)\n",
    "    \n",
    "    return concatinated_article_objects\n",
    "        \n",
    "#\n",
    "# Testing\n",
    "#\n",
    "\n",
    "# source\n",
    "#selected_source_id = name_id_dict['Fox News']\n",
    "    \n",
    "# number of documents\n",
    "#num_documents = 50\n",
    "\n",
    "# example -- set include full article text to True for scrapping the actual site\n",
    "#all_article_objects = get_all_articles(selected_source_id, api_key, max_articles=num_documents, include_full_article_text=True)\n",
    "\n",
    "#simple_article_objects = article_objects_to_single_document_strings(all_article_objects)\n",
    "#simple_article_objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>bias</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>breitbart-news</th>\n",
       "      <td>Breitbart News</td>\n",
       "      <td>conservative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cnn</th>\n",
       "      <td>CNN</td>\n",
       "      <td>liberal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fox-news</th>\n",
       "      <td>Fox News</td>\n",
       "      <td>conservative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>msnbc</th>\n",
       "      <td>MSNBC</td>\n",
       "      <td>liberal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>reuters</th>\n",
       "      <td>Reuters</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          name          bias\n",
       "id                                          \n",
       "breitbart-news  Breitbart News  conservative\n",
       "cnn                        CNN       liberal\n",
       "fox-news              Fox News  conservative\n",
       "msnbc                    MSNBC       liberal\n",
       "reuters                Reuters       neutral"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#\n",
    "# Selected sources and bias labels (conservative, nuetral, liberal)\n",
    "# \n",
    "# Selected sources 'Breitbart News', 'Fox News', 'Reuters', 'MSNBC', and 'CNN' - (conservative, conservative, neutral, liberal, liberal)\n",
    "#\n",
    "selected_source_ids = ['cnn', 'msnbc', 'reuters', 'fox-news', 'breitbart-news']\n",
    "\n",
    "# bias labels python dictonary\n",
    "id_bias_dict = {\n",
    "    'cnn' : 'liberal',\n",
    "    'msnbc' : 'liberal',\n",
    "    'reuters' : 'neutral',\n",
    "    'fox-news' : 'conservative',\n",
    "    'breitbart-news' : 'conservative'\n",
    "}\n",
    "\n",
    "id_bias_dict\n",
    "\n",
    "# pandas data frame version\n",
    "id_bias_df = pd.DataFrame.from_dict(data=id_bias_dict, orient='index', columns=['bias'])\n",
    "id_bias_df.index.name = 'id'\n",
    "id_bias_df\n",
    "\n",
    "# join name id dataframe and bias id dataframe\n",
    "selected_news_sources_df = pd.merge(id_name_df, id_bias_df, on='id')\n",
    "selected_news_sources_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "breitbart-news\n",
      "number of articles collected: 100\n",
      "cnn\n",
      "number of articles collected: 98\n",
      "number of articles collected: 100\n",
      "fox-news\n",
      "number of articles collected: 100\n",
      "msnbc\n",
      "number of articles collected: 100\n",
      "reuters\n",
      "number of articles collected: 100\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>frequency</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>term</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>jack</th>\n",
       "      <td>720</td>\n",
       "      <td>breitbart-news</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nicklaus</th>\n",
       "      <td>929</td>\n",
       "      <td>breitbart-news</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>backs</th>\n",
       "      <td>139</td>\n",
       "      <td>breitbart-news</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>woods</th>\n",
       "      <td>1551</td>\n",
       "      <td>breitbart-news</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>on</th>\n",
       "      <td>964</td>\n",
       "      <td>breitbart-news</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>trump</th>\n",
       "      <td>1427</td>\n",
       "      <td>breitbart-news</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>comments</th>\n",
       "      <td>298</td>\n",
       "      <td>breitbart-news</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>you</th>\n",
       "      <td>1563</td>\n",
       "      <td>breitbart-news</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>respect</th>\n",
       "      <td>1161</td>\n",
       "      <td>breitbart-news</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>the</th>\n",
       "      <td>1376</td>\n",
       "      <td>breitbart-news</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>office</th>\n",
       "      <td>958</td>\n",
       "      <td>breitbart-news</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>golf</th>\n",
       "      <td>588</td>\n",
       "      <td>breitbart-news</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>legend</th>\n",
       "      <td>780</td>\n",
       "      <td>breitbart-news</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lent</th>\n",
       "      <td>783</td>\n",
       "      <td>breitbart-news</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>support</th>\n",
       "      <td>1334</td>\n",
       "      <td>breitbart-news</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>to</th>\n",
       "      <td>1399</td>\n",
       "      <td>breitbart-news</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tiger</th>\n",
       "      <td>1394</td>\n",
       "      <td>breitbart-news</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wednesday</th>\n",
       "      <td>1516</td>\n",
       "      <td>breitbart-news</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>agreeing</th>\n",
       "      <td>50</td>\n",
       "      <td>breitbart-news</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>with</th>\n",
       "      <td>1547</td>\n",
       "      <td>breitbart-news</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>decision</th>\n",
       "      <td>380</td>\n",
       "      <td>breitbart-news</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>of</th>\n",
       "      <td>953</td>\n",
       "      <td>breitbart-news</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>presidency</th>\n",
       "      <td>1057</td>\n",
       "      <td>breitbart-news</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>and</th>\n",
       "      <td>73</td>\n",
       "      <td>breitbart-news</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>not</th>\n",
       "      <td>942</td>\n",
       "      <td>breitbart-news</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>get</th>\n",
       "      <td>578</td>\n",
       "      <td>breitbart-news</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>involved</th>\n",
       "      <td>706</td>\n",
       "      <td>breitbart-news</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>in</th>\n",
       "      <td>674</td>\n",
       "      <td>breitbart-news</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>political</th>\n",
       "      <td>1035</td>\n",
       "      <td>breitbart-news</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>discussions</th>\n",
       "      <td>420</td>\n",
       "      <td>breitbart-news</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nowhere</th>\n",
       "      <td>910</td>\n",
       "      <td>reuters</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hear</th>\n",
       "      <td>595</td>\n",
       "      <td>reuters</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>leftist</th>\n",
       "      <td>762</td>\n",
       "      <td>reuters</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nicolas</th>\n",
       "      <td>897</td>\n",
       "      <td>reuters</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>maduro</th>\n",
       "      <td>803</td>\n",
       "      <td>reuters</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tell</th>\n",
       "      <td>1330</td>\n",
       "      <td>reuters</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>remote</th>\n",
       "      <td>1102</td>\n",
       "      <td>reuters</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hamlet</th>\n",
       "      <td>582</td>\n",
       "      <td>reuters</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300</th>\n",
       "      <td>13</td>\n",
       "      <td>reuters</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>souls</th>\n",
       "      <td>1230</td>\n",
       "      <td>reuters</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>perched</th>\n",
       "      <td>975</td>\n",
       "      <td>reuters</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>edge</th>\n",
       "      <td>421</td>\n",
       "      <td>reuters</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>innovation</th>\n",
       "      <td>666</td>\n",
       "      <td>reuters</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>idris</th>\n",
       "      <td>634</td>\n",
       "      <td>reuters</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>elba</th>\n",
       "      <td>426</td>\n",
       "      <td>reuters</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>camera</th>\n",
       "      <td>206</td>\n",
       "      <td>reuters</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>crime</th>\n",
       "      <td>312</td>\n",
       "      <td>reuters</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>drama</th>\n",
       "      <td>400</td>\n",
       "      <td>reuters</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yardie</th>\n",
       "      <td>1483</td>\n",
       "      <td>reuters</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>actor</th>\n",
       "      <td>37</td>\n",
       "      <td>reuters</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>goes</th>\n",
       "      <td>561</td>\n",
       "      <td>reuters</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>directorial</th>\n",
       "      <td>373</td>\n",
       "      <td>reuters</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>debut</th>\n",
       "      <td>347</td>\n",
       "      <td>reuters</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>young</th>\n",
       "      <td>1490</td>\n",
       "      <td>reuters</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>perspective</th>\n",
       "      <td>978</td>\n",
       "      <td>reuters</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>violence</th>\n",
       "      <td>1434</td>\n",
       "      <td>reuters</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>when</th>\n",
       "      <td>1464</td>\n",
       "      <td>reuters</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>suffering</th>\n",
       "      <td>1282</td>\n",
       "      <td>reuters</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>surge</th>\n",
       "      <td>1294</td>\n",
       "      <td>reuters</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>knife</th>\n",
       "      <td>731</td>\n",
       "      <td>reuters</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7178 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             frequency              id\n",
       "term                                  \n",
       "jack               720  breitbart-news\n",
       "nicklaus           929  breitbart-news\n",
       "backs              139  breitbart-news\n",
       "woods             1551  breitbart-news\n",
       "on                 964  breitbart-news\n",
       "trump             1427  breitbart-news\n",
       "comments           298  breitbart-news\n",
       "you               1563  breitbart-news\n",
       "respect           1161  breitbart-news\n",
       "the               1376  breitbart-news\n",
       "office             958  breitbart-news\n",
       "golf               588  breitbart-news\n",
       "legend             780  breitbart-news\n",
       "lent               783  breitbart-news\n",
       "support           1334  breitbart-news\n",
       "to                1399  breitbart-news\n",
       "tiger             1394  breitbart-news\n",
       "wednesday         1516  breitbart-news\n",
       "agreeing            50  breitbart-news\n",
       "with              1547  breitbart-news\n",
       "decision           380  breitbart-news\n",
       "of                 953  breitbart-news\n",
       "presidency        1057  breitbart-news\n",
       "and                 73  breitbart-news\n",
       "not                942  breitbart-news\n",
       "get                578  breitbart-news\n",
       "involved           706  breitbart-news\n",
       "in                 674  breitbart-news\n",
       "political         1035  breitbart-news\n",
       "discussions        420  breitbart-news\n",
       "...                ...             ...\n",
       "nowhere            910         reuters\n",
       "hear               595         reuters\n",
       "leftist            762         reuters\n",
       "nicolas            897         reuters\n",
       "maduro             803         reuters\n",
       "tell              1330         reuters\n",
       "remote            1102         reuters\n",
       "hamlet             582         reuters\n",
       "300                 13         reuters\n",
       "souls             1230         reuters\n",
       "perched            975         reuters\n",
       "edge               421         reuters\n",
       "innovation         666         reuters\n",
       "idris              634         reuters\n",
       "elba               426         reuters\n",
       "camera             206         reuters\n",
       "crime              312         reuters\n",
       "drama              400         reuters\n",
       "yardie            1483         reuters\n",
       "actor               37         reuters\n",
       "goes               561         reuters\n",
       "directorial        373         reuters\n",
       "debut              347         reuters\n",
       "young             1490         reuters\n",
       "perspective        978         reuters\n",
       "violence          1434         reuters\n",
       "when              1464         reuters\n",
       "suffering         1282         reuters\n",
       "surge             1294         reuters\n",
       "knife              731         reuters\n",
       "\n",
       "[7178 rows x 2 columns]"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#\n",
    "# scrape from selected sources\n",
    "#\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "\n",
    "# sources\n",
    "selected_source_ids = list(selected_news_sources_df.index.values)\n",
    "    \n",
    "# number of documents\n",
    "num_documents_per_source = 100\n",
    "\n",
    "def scrape_from_sources_bag_of_words(api_key, selected_source_ids, num_documents_per_source):\n",
    "    \n",
    " \n",
    "    frames = []\n",
    "   \n",
    "    for sid in selected_source_ids:\n",
    "        \n",
    "        #set include full article text to True for scrapping the actual site\n",
    "        all_article_objects = get_all_articles(sid, api_key, max_articles=num_documents_per_source, include_full_article_text=False)\n",
    "\n",
    "        simple_article_objects = article_objects_to_single_document_strings(all_article_objects)\n",
    "        source_articles = [x['article_string'] for x in simple_article_objects]\n",
    "\n",
    "        count_vect = CountVectorizer()\n",
    "        bag_words = count_vect.fit_transform(source_articles)\n",
    "        \n",
    "        vocab_df = pd.DataFrame.from_dict(data=count_vect.vocabulary_, orient='index', columns=['frequency'])\n",
    "        vocab_df.index.name = 'term'\n",
    "        vocab_df['id'] = sid\n",
    "        \n",
    "        frames.append(vocab_df)\n",
    "    \n",
    "    source_id_vocab_df = pd.concat(frames)\n",
    "    \n",
    "    return source_id_vocab_df\n",
    "    \n",
    "\n",
    "# scrape from all selected sources and store as term frequency id in panda dataframe\n",
    "scrape_from_sources_bag_of_words(api_key, selected_source_ids, num_documents_per_source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
