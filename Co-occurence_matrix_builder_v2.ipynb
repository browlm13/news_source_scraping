{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = [\"\"\"God made mud. God got lonesome. So God said to some of the mud, \\\"Sit up!\\\" \n",
    "        \\\"See all I\\'ve made,\\\" said God, \\\"the hills, the sea, the sky, the stars.\\\"\n",
    "        And I was some of the mud that got to sit up and look around. Lucky me, lucky mud.\"\"\",\n",
    "        \n",
    "        \"\"\"I, mud, sat up and saw what a nice job God had done.\n",
    "        Nice going, God. Nobody but you could have done it, God! I certainly couldn\\'t have.\n",
    "        I feel very unimportant compared to You.\n",
    "        The only way I can feel the least bit important is to\n",
    "        think of all the mud that didn't even get to sit up and look around.\n",
    "        I got so much, and most mud got so little. Thank you for the honor!\"\"\",\n",
    "        \n",
    "        \"\"\"Now mud lies down again and goes to sleep. What memories for mud to have!\n",
    "        What interesting other kinds of sitting-up mud I met! I loved everything I saw! Good night.\n",
    "        I will go to heaven now. I can hardly wait... To find out for certain what my wampeter was...\n",
    "        And who was in my karass...And all the good things our karass did for you. Amen.\"\"\"]\n",
    "\n",
    "corpus = '\\n\\n'.join(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Build Co-occurence Matrix, A\n",
    "#\n",
    "\n",
    "import itertools\n",
    "\n",
    "# external\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.util import ngrams\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "def process_text(text):\n",
    "\t\"\"\" return list of lowercase alphabetic words from text \"\"\"\n",
    "\ttokenizer = RegexpTokenizer(r'\\w+')\n",
    "\treturn tokenizer.tokenize(text.lower())\n",
    "\n",
    "def ngram_tupples(corpus, n):\n",
    "\t\"\"\" Create ngram tupples by sentence. Where n is the distance between words in a sentence. \"\"\"\n",
    "\tsentences = sent_tokenize(corpus)\n",
    "\n",
    "\tpairs = []\n",
    "\tfor s in sentences:\n",
    "\t\tunique_tokens = process_text(s)\n",
    "\t\tpairs.extend(ngrams(unique_tokens,n))\n",
    "\n",
    "\treturn pairs\n",
    "\n",
    "def get_unique_words(corpus):\n",
    "\treturn list(set(process_text(corpus)))\n",
    "\n",
    "def w2id_id2w_maps(unique_words):\n",
    "\t\"\"\" return both dictonaries for mapping between words and ids \"\"\"\n",
    "\tid2w = {i:w for i,w in enumerate(unique_words)}\n",
    "\tw2id = {w:i for i,w in id2w.items()}\n",
    "\treturn w2id, id2w\n",
    "\n",
    "def ngram_inc_amt(n):\n",
    "\t\"\"\" return float for increment weight of pair occurence n distance appart. \\nWeight increment ~ 1/n \"\"\"\n",
    "\treturn 1/float(n**2)\n",
    "\n",
    "def words2ids(words, w2id):\n",
    "\t\"\"\" return list of ids inplace of list of words using w2id dictionary \"\"\"\n",
    "\treturn [w2id[w] for w in words]\n",
    "\n",
    "def cooccurence_pair_of_distance(sentence_list, d):\n",
    "    \"\"\" return list of unique coocurence pairs of distace d \"\"\"\n",
    "\n",
    "    all_ngrams = ngrams(sentence_list,d)\n",
    "\n",
    "    all_pairs = []\n",
    "    for t in all_ngrams:\n",
    "        if len(t) > 1:\n",
    "            all_pairs.extend(list(itertools.combinations(t, 2)))\n",
    "\n",
    "    return list(set(all_pairs))\n",
    "\n",
    "def break_corpus(corpus):\n",
    "    \"\"\" Build Cooccurence Matrix. Return A, n, w2id, id2w \"\"\"\n",
    "\n",
    "    unique_words = get_unique_words(corpus)\n",
    "    n = len(unique_words)\n",
    "    w2id, id2w = w2id_id2w_maps(unique_words)\n",
    "\n",
    "    #create empty cooccurence matrix\n",
    "    #A = np.zeros([n,n],np.float32)\n",
    "    A = np.ones([n,n],np.float32)\n",
    "\n",
    "    #compute cooccurence matrix\n",
    "    sentences = sent_tokenize(corpus)\n",
    "    for s in sentences:\n",
    "        s = process_text(s)\n",
    "        max_distance = len(s) + 1\n",
    "        s = [w2id[w] for w in s]\t#convert words to ids\n",
    "\n",
    "        for d in range(2,max_distance):\n",
    "            pairs = cooccurence_pair_of_distance(s, d)\n",
    "\n",
    "            #update cooccurence matrix for each pair\n",
    "            for p in pairs:\n",
    "                A[p[0],p[1]] += ngram_inc_amt(d)\n",
    "                A[p[1],p[0]] += ngram_inc_amt(d)\n",
    "\n",
    "    return A, n, w2id, id2w\n",
    "\n",
    "A, n, w2id, id2w = break_corpus(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "\tNormalize and clean text methods\n",
    "\"\"\"\n",
    "import logging\n",
    "import string\n",
    "import math\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk import FreqDist\n",
    "from nltk.tag import pos_tag\n",
    "\n",
    "# logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def is_stopword(word):\n",
    "\t\"\"\" Return True of word is in stop word list \"\"\"\n",
    "\tstop_words = nltk.corpus.stopwords.words('english')\n",
    "\treturn word in stop_words\n",
    "\n",
    "def is_punctuation(word):\n",
    "\treturn len(word) == 1 and word in string.punctuation\n",
    "\n",
    "def is_number(word):\n",
    "\ttry:\n",
    "\t\tfloat(word)\n",
    "\t\treturn True\n",
    "\texcept ValueError:\n",
    "\t\tlogger.debug('ValueError is_number')\n",
    " \n",
    "\ttry:\n",
    "\t\timport unicodedata\n",
    "\t\tunicodedata.numeric(word)\n",
    "\t\treturn True\n",
    "\texcept (TypeError, ValueError):\n",
    "\t\tlogger.debug('ValueError is_number')\n",
    "\t \n",
    "\treturn False\n",
    "\n",
    "def is_shorter(word,n=3):\n",
    "\tif len(word) < n:\n",
    "\t\treturn True\n",
    "\treturn False\n",
    "\n",
    "def stem(word):\n",
    "\tps = PorterStemmer()\n",
    "\treturn ps.stem(word)\n",
    "\n",
    "def clean_word(raw_word):\n",
    "\t\"\"\" Takes string converts to lower case, stems \n",
    "\tand returns empty string if word is stop word, \n",
    "\tpunctation or is less than 3 characters long \"\"\"\n",
    "\n",
    "\traw_word = raw_word.lower()\n",
    "\tif is_stopword(raw_word) or is_punctuation(raw_word) or is_shorter(raw_word) or is_number(raw_word):\n",
    "\t\tword = \"\"\n",
    "\telse:\n",
    "\t\tword = stem(raw_word)\n",
    "\treturn word\n",
    "\n",
    "def remove_short_and_stopwords(token_list):\n",
    "\tfiltered_token_list = []\n",
    "\tfor t in token_list:\n",
    "\t\tif is_stopword(t) or is_punctuation(t) or is_shorter(t) or is_number(t):pass\n",
    "\t\telse: filtered_token_list.append(t)\n",
    "\treturn filtered_token_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['said',\n",
       " 'made',\n",
       " 'done',\n",
       " 'job',\n",
       " 'nice',\n",
       " 'going',\n",
       " 'got',\n",
       " 'hills',\n",
       " 'mud',\n",
       " 'sea',\n",
       " 'lonesome',\n",
       " 'saw',\n",
       " 'could',\n",
       " 'see',\n",
       " 'sky',\n",
       " 'stars',\n",
       " 'sat',\n",
       " 'sit',\n",
       " 'nobody',\n",
       " 'honor',\n",
       " 'sleep',\n",
       " 'wait',\n",
       " 'least',\n",
       " 'god',\n",
       " 'met',\n",
       " 'important',\n",
       " 'amen',\n",
       " 'memories',\n",
       " 'get',\n",
       " 'around',\n",
       " 'sitting',\n",
       " 'bit',\n",
       " 'hardly',\n",
       " 'goes',\n",
       " 'interesting',\n",
       " 'feel',\n",
       " 'heaven',\n",
       " 'kinds',\n",
       " 'certain',\n",
       " 'way',\n",
       " 'everything',\n",
       " 'things',\n",
       " 'lucky',\n",
       " 'wampeter',\n",
       " 'unimportant',\n",
       " 'much',\n",
       " 'loved',\n",
       " 'thank',\n",
       " 'look',\n",
       " 'find',\n",
       " 'little',\n",
       " 'certainly',\n",
       " 'good',\n",
       " 'think',\n",
       " 'night',\n",
       " 'even',\n",
       " 'lies',\n",
       " 'compared',\n",
       " 'karass']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def rank_most_related_words(A, w2id, id2w, w1):\n",
    "    col = A[:,w2id[w1]].tolist()\n",
    "\n",
    "    list_of_index_value_tuples = list(zip(list(range(len(col))),col))\n",
    "    #(colindex,value)\n",
    "    sorted_indexs_value_tuples = sorted(list_of_index_value_tuples, key=lambda x: x[1])\n",
    "    sorted_indexs_value_tuples.reverse()\n",
    "    indexs, values = zip(*sorted_indexs_value_tuples)\n",
    "    ranked_words = [id2w[i] for i in indexs]\n",
    "    return ranked_words\n",
    "\n",
    "# rank_most_related_words(A,w2id, id2w, 'aa')\n",
    "\n",
    "def related_words_list_filtered_decending(A, w2id, id2w, w1):\n",
    "    raw_related_words = rank_most_related_words(A, w2id, id2w, w1)\n",
    "    filtered_related_words = remove_short_and_stopwords(raw_related_words)\n",
    "    return filtered_related_words\n",
    "\n",
    "related_words_list_filtered_decending(A,w2id, id2w, 'god')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Symmetric Positive Definite Decomposition:\n",
      "\n",
      "\n",
      "\n",
      "A:\n",
      "[[1.        1.        1.        ... 1.        1.        1.       ]\n",
      " [1.        1.        1.        ... 1.        1.        1.       ]\n",
      " [1.        1.        1.1984842 ... 1.        1.        1.       ]\n",
      " ...\n",
      " [1.        1.        1.        ... 1.        1.        1.       ]\n",
      " [1.        1.        1.        ... 1.        1.        1.       ]\n",
      " [1.        1.        1.        ... 1.        1.        1.       ]]\n",
      "\n",
      "\n",
      "A2 (modified into SPD):\n",
      "\n",
      "[[102.88062   1.        1.      ...   1.        1.        1.     ]\n",
      " [  1.      101.42222   1.      ...   1.        1.        1.     ]\n",
      " [  1.        1.      106.71744 ...   1.        1.        1.     ]\n",
      " ...\n",
      " [  1.        1.        1.      ... 102.2087    1.        1.     ]\n",
      " [  1.        1.        1.      ...   1.      100.81972   1.     ]\n",
      " [  1.        1.        1.      ...   1.        1.      101.70223]]\n",
      "\n",
      "WWt:\n",
      "\n",
      "[[102.88062      0.9999993    0.99999917 ...   0.99999845   0.99999994\n",
      "    1.0000001 ]\n",
      " [  0.9999993  101.42221      0.99999946 ...   0.9999988    1.\n",
      "    0.99999905]\n",
      " [  0.99999917   0.99999946 106.71747    ...   0.9999996    0.9999995\n",
      "    0.9999994 ]\n",
      " ...\n",
      " [  0.99999845   0.9999988    0.9999996  ... 102.208694     0.9999996\n",
      "    1.0000012 ]\n",
      " [  0.99999994   1.           0.9999995  ...   0.9999996  100.81974\n",
      "    0.9999991 ]\n",
      " [  1.0000001    0.99999905   0.9999994  ...   1.0000012    0.9999991\n",
      "  101.702225  ]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:76: RuntimeWarning: invalid value encountered in sqrt\n"
     ]
    }
   ],
   "source": [
    "#internal\n",
    "import random\n",
    "\n",
    "#external\n",
    "import numpy as np\n",
    "\n",
    "\"\"\"\n",
    "Matrix Decomposition\n",
    "\tMethod 1) Singular Value Decomposition of Forced Square Symmetric Positive Definite Matrix\n",
    "\tMethod 2) Eigen Decomposition of Forced Square Symmetric Positive Matrix\n",
    "\tMethod 3) Stocastic Gradient Decent Decomposition\n",
    "\"\"\"\n",
    "\n",
    "# check symmetry of matrix up to some tolerance\n",
    "def check_symmetric(a, tol=1e-8):\n",
    "    return np.allclose(a, a.T, atol=tol)\n",
    "\n",
    "#force square symetric matrix to symetric positive definite matrix\n",
    "def to_positive_definite(S):\n",
    "    \"\"\" Take absolute value of S and update diagnol entries to make a diagonally dominant matrix with diagonal entries greater than 0. \"\"\"\n",
    "    # take absolute value of S\n",
    "    S = np.absolute(S)\n",
    "\n",
    "    # Sum rows in S\n",
    "    new_diagonals = S.sum(axis=1)\n",
    "\n",
    "    #replace diagnols in S\n",
    "    np.fill_diagonal(S, new_diagonals)\n",
    "\n",
    "    return S\n",
    "\n",
    "\"\"\"\n",
    "####################################################################################\n",
    "# Singular Value Decomposition of Forced Square Symmetric Positive Definite Matrix #\n",
    "####################################################################################\n",
    "1.) Force Cooccurence Matrix A to Positive Definite Matrix\n",
    "    \" A diagonally dominant(by rows) symetric matrix with diagonal elements all greater than zero is positive definite.\"\n",
    "    Take symmetric matrix and make diagonally dominant with diagnonal entries greater than 0\n",
    "2.) Square Symmetric Positive Definite Matrix Decomposition\n",
    "    \" If A is positive definite, then A = QLQt = UDV (where U=V=Q and L=D) can be written as A = WWt where W = Qsqrt(L) \"\n",
    "    -SVD: A=UDV, W = Vsqrt(diagnol(D))\n",
    "    Find V and D from singular value decomposition of A\n",
    "    return W = Vsqrt(D)\n",
    "overview of code:\n",
    "    #\n",
    "    # force symmetric matrix to positive definite matrix\n",
    "    #\n",
    "    # take absolute value of A\n",
    "    A = np.absolute(A)\n",
    "    # Sum rows in a\n",
    "    new_diagonals = A.sum(axis=1)\n",
    "    #replace diagnols in A\n",
    "    np.fill_diagonal(A, new_diagonals)\n",
    "    #\n",
    "    # decompose positive definite matrix\n",
    "    #\n",
    "    # singular value decomposition\n",
    "    U, D, V = np.linalg.svd(A, full_matrices=False)\n",
    "    #\n",
    "    # compute W from V and D of singular value decomposition\n",
    "    #\n",
    "    # Create matrix W = Vtsqrt(diagnol(D)) #why Vt?\n",
    "    W = np.dot(np.transpose(V), np.sqrt(np.diag(D)))\n",
    "    #A = WWt\n",
    "\"\"\"\n",
    "\n",
    "def svd_spd_decomposition(P):\n",
    "    \"\"\" return M such that P = MMt, where matrix parameter P is SPD \"\"\"\n",
    "    # Assert Matrix P is symetric\n",
    "    assert check_symmetric(P)\n",
    "\n",
    "    # singular value decomposition\n",
    "    U, D, V = np.linalg.svd(P, full_matrices=False)\n",
    "\n",
    "    # Create matrix W = Vtsqrt(diagnol(D)) #why Vt?\n",
    "    M = np.dot(np.transpose(V), np.sqrt(np.diag(D)))\n",
    "\n",
    "    return M\n",
    "\n",
    "def spd_decomposition(S):\n",
    "\t\"\"\" Force Cooccurence Matrix A to Positive Definite Matrix and decompose into W such that A = WWt. \"\"\"\n",
    "\tP = to_positive_definite(S)\n",
    "\tM = svd_spd_decomposition(P)\n",
    "\treturn M\n",
    "\n",
    "#\n",
    "# Perform Symmetric Positive Definite Decomposition\n",
    "#\n",
    "\n",
    "W = spd_decomposition(A)\n",
    "\n",
    "print(\"\\n\\n\\nSymmetric Positive Definite Decomposition:\\n\")\n",
    "print(\"\\n\\nA:\")\n",
    "print (A)\n",
    "print(\"\\n\\nA2 (modified into SPD):\\n\")\n",
    "print(to_positive_definite(A))\n",
    "print(\"\\nWWt:\\n\")\n",
    "print(np.dot(W, np.transpose(W)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "term: was\n",
      "['wampeter', 'karass', 'certain', 'mud', 'find', 'good', 'got', 'things', 'wait', 'hardly', 'sit', 'look', 'around', 'honor', 'stars', 'sleep', 'going', 'least', 'god', 'see']\n",
      "\n",
      "term: important\n",
      "['bit', 'least', 'think', 'feel', 'way', 'mud', 'even', 'get', 'sit', 'look', 'around', 'honor', 'stars', 'sleep', 'going', 'wait', 'god', 'see', 'met', 'job']\n",
      "\n",
      "term: can\n",
      "['hardly', 'feel', 'wait', 'way', 'least', 'find', 'bit', 'important', 'certain', 'think', 'wampeter', 'mud', 'even', 'karass', 'get', 'sit', 'good', 'things', 'look', 'around']\n",
      "\n",
      "term: sea\n",
      "['sky', 'hills', 'stars', 'god', 'said', 'made', 'see', 'honor', 'sleep', 'going', 'wait', 'least', 'met', 'job', 'important', 'nice', 'amen', 'memories', 'sat', 'get']\n",
      "\n",
      "term: did\n",
      "['karass', 'things', 'good', 'wampeter', 'certain', 'find', 'wait', 'hardly', 'honor', 'stars', 'sleep', 'going', 'least', 'god', 'see', 'met', 'job', 'important', 'nice', 'amen']\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "n = 5 # sample size\n",
    "m = 20 # top m most related words\n",
    "random_indexs = random.sample(range(0, len(id2w)-1), n)\n",
    "corpus_terms = [id2w[i] for i in random_indexs]\n",
    "\n",
    "#corpus_terms = ['republican','flynn','trump','2016','lied','biblical','drug','communist','criminal','russia','chinese','inauguration','obama','nuclear','spine','twitter','students','unanimous','hillary','peace','deal', 'clearances','financial','failed','prescription','unprecedented','election','ship','taliban','military']\n",
    "#corpus_terms = ['republican','ship','drug','communist','taliban','flynn','russia','inauguration','trump','obama','spine','2016','twitter','unprecedented','election','unanimous','hillary','prescription','peace','financial']\n",
    "\n",
    "# list of lists where the first index is a corpus term and the trailing indexs are its ranked related words\n",
    "list_of_related_word_lists = []\n",
    "for t in corpus_terms:\n",
    "    print('\\nterm: %s' % t)\n",
    "    related_terms = related_words_list_filtered_decending(A, w2id, id2w, t)\n",
    "    print(related_terms[:m])\n",
    "\n",
    "    list_of_related_word_lists.append(related_terms[:m])\n",
    "\n",
    "np_related_words = np.array(list_of_related_word_lists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
