{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = ['aa aa bb cc.',\n",
    "        'cc dd ee ff.',\n",
    "        'ff ff gg aa']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 2 2 0 0 2 1]\n",
      " [2 0 1 0 0 0 0]\n",
      " [2 1 0 1 1 1 0]\n",
      " [0 0 1 0 1 1 0]\n",
      " [0 0 1 1 0 1 0]\n",
      " [2 0 1 1 1 0 2]\n",
      " [1 0 0 0 0 2 0]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "count_model = CountVectorizer(ngram_range=(1,1)) # default unigram model\n",
    "X = count_model.fit_transform(docs)\n",
    "# X[X > 0] = 1 # run this line if you don't want extra within-text cooccurence (see below)\n",
    "Xc = (X.T * X) # this is co-occurrence matrix in sparse csr format\n",
    "Xc.setdiag(0) # sometimes you want to fill same word cooccurence to 0\n",
    "print(Xc.todense()) # print out matrix in dense format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['aa', 'bb', 'cc', 'dd', 'ee', 'ff', 'gg']"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_model.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'aa': 0, 'bb': 1, 'cc': 2, 'dd': 3, 'ee': 4, 'ff': 5, 'gg': 6}"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_model.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 1 0 0 0 0 0 0]\n",
      " [1 0 1 0 0 0 0 0 0]\n",
      " [1 1 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 1 1 0 0 0]\n",
      " [0 0 0 1 0 1 0 0 0]\n",
      " [0 0 0 1 1 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 1 1]\n",
      " [0 0 0 0 0 0 1 0 1]\n",
      " [0 0 0 0 0 0 1 1 0]]\n"
     ]
    }
   ],
   "source": [
    "# note this method of computing bigrams treats the document as a bag of words and does not\n",
    "# preserve distance\n",
    "bigram_vectorizer = CountVectorizer(ngram_range=(2, 2)) # by saying 2,2 you are telling you only want pairs of 2 words\n",
    "\n",
    "X = bigram_vectorizer.fit_transform(docs)\n",
    "# X[X > 0] = 1 # run this line if you don't want extra within-text cooccurence (see below)\n",
    "Xc = (X.T * X) # this is co-occurrence matrix in sparse csr format\n",
    "Xc.setdiag(0) # sometimes you want to fill same word cooccurence to 0\n",
    "print(Xc.todense()) # print out matrix in dense format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'aa aa': 0,\n",
       " 'aa bb': 1,\n",
       " 'bb cc': 2,\n",
       " 'cc dd': 3,\n",
       " 'dd ee': 4,\n",
       " 'ee ff': 5,\n",
       " 'ff ff': 6,\n",
       " 'ff gg': 7,\n",
       " 'gg aa': 8}"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram_vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 1 0 0 0 0 0 0]\n",
      " [1 0 1 0 0 0 0 0 0]\n",
      " [1 1 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 1 1 0 0 0]\n",
      " [0 0 0 1 0 1 0 0 0]\n",
      " [0 0 0 1 1 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 1 1]\n",
      " [0 0 0 0 0 0 1 0 1]\n",
      " [0 0 0 0 0 0 1 1 0]]\n"
     ]
    }
   ],
   "source": [
    "# note this method of computing bigrams treats the document as a bag of words and does not\n",
    "# preserve distance\n",
    "bigram_vectorizer = CountVectorizer(ngram_range=(2, 2)) # by saying 2,2 you are telling you only want pairs of 2 words\n",
    "\n",
    "X = bigram_vectorizer.fit_transform(docs)\n",
    "# X[X > 0] = 1 # run this line if you don't want extra within-text cooccurence (see below)\n",
    "Xc = (X.T * X) # this is co-occurrence matrix in sparse csr format\n",
    "Xc.setdiag(0) # sometimes you want to fill same word cooccurence to 0\n",
    "print(Xc.todense()) # print out matrix in dense format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'aa aa': 0, 'bb cc': 1, 'cc dd': 2, 'ee ff': 3, 'ff ff': 4, 'gg aa': 5}\n",
      "{'aa bb': 0, 'dd ee': 1, 'ff gg': 2}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<3x9 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 9 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# finding only adjacent bigrams\n",
    "# defualt pattern : '(?u)\\\\b\\\\w\\\\w+\\\\b'\n",
    "# new patterns : \n",
    "# starting at first word all sucessive pairs: \\w+(?:\\W+)\\w+\n",
    "# starting at second word all sucsessive pairs: r'(?u)(?:\\w+\\W+)(\\w+(?:\\W+)\\w+)'\n",
    "adjacent_word_pattern = r'(?u)\\w+(?:\\W+)\\w+' # starting from first 1st word\n",
    "adjacent_word_pattern_offset_1 = r'(?u)(?:\\w+\\W+)(\\w+(?:\\W+)\\w+)' # starting from first 2md word\n",
    "\n",
    "A = CountVectorizer(token_pattern=adjacent_word_pattern)\n",
    "A.fit_transform(docs)\n",
    "print(A.vocabulary_)\n",
    "\n",
    "B = CountVectorizer(token_pattern=adjacent_word_pattern_offset_1)\n",
    "B.fit_transform(docs)\n",
    "print(B.vocabulary_)\n",
    "\n",
    "# combine the two sets\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "adjacent_bigram_vectorize = FeatureUnion([('CountVectorizer', A),('CountVect', B)])\n",
    "adjacent_bigram_vectorize.fit_transform(docs)\n",
    "\n",
    "#print(adjacent_bigram_vectorize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'aa aa': 0,\n",
       " 'aa bb': 1,\n",
       " 'bb cc': 2,\n",
       " 'cc dd': 3,\n",
       " 'dd ee': 4,\n",
       " 'ee ff': 5,\n",
       " 'ff ff': 6,\n",
       " 'ff gg': 7,\n",
       " 'gg aa': 8}"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram_vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'aa': 0, 'bb': 1, 'cc': 2, 'dd': 3, 'ee': 4, 'ff': 5, 'gg': 6}"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_model.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['aa', 'bb', 'cc', 'dd', 'ee', 'ff', 'gg']"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#unique_words = list(count_model.vocabulary_.keys())\n",
    "unique_words = count_model.get_feature_names()\n",
    "unique_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'aa': 0, 'bb': 1, 'cc': 2, 'dd': 3, 'ee': 4, 'ff': 5, 'gg': 6}"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_2_index = dict(zip(unique_words,range(len(unique_words))))\n",
    "word_2_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'aa', 1: 'bb', 2: 'cc', 3: 'dd', 4: 'ee', 5: 'ff', 6: 'gg'}"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_2_word = {v:k for k,v in word_2_index.items()}\n",
    "index_2_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# build empty co occurence matrix\n",
    "n = len(unique_words)\n",
    "co_mat = np.zeros(shape=(n,n))\n",
    "co_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.,  2.,  0.,  0.,  0.,  0.,  9.],\n",
       "       [ 2.,  0.,  3.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  3.,  0.,  4.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  4.,  0.,  5.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  5.,  0.,  6.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  6., 14.,  8.],\n",
       "       [ 9.,  0.,  0.,  0.,  0.,  8.,  0.]])"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fill cooccurence matrix\n",
    "for bigram,num_occurences_minus_1 in bigram_vectorizer.vocabulary_.items():\n",
    "    \n",
    "    # correct for counted number of occurences being 1 less than desired value\n",
    "    num_occurences = num_occurences_minus_1 + 1\n",
    "    \n",
    "    # the bigram is tokens seperated by a space, num_occurences is the number of occurences of that bigram\n",
    "    bigram_tokens = bigram.split(' ')\n",
    "\n",
    "    # get the index of each token\n",
    "    index_token_1 = word_2_index[bigram_tokens[0]]\n",
    "    index_token_2 = word_2_index[bigram_tokens[1]]\n",
    "    \n",
    "    # lazy make symetric\n",
    "    co_mat[index_token_1, index_token_2] += num_occurences\n",
    "    co_mat[index_token_2, index_token_1] += num_occurences\n",
    "    \n",
    "    \n",
    "co_mat\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[1, 1, 1, 1, 1, 1, 1, 1, 1]], dtype=int64)"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "Y = bigram_vectorizer.fit_transform(docs)\n",
    "sum_occ = np.sum(Y.todense(),axis=0)\n",
    "#Yc = (Y.T * Y) # this is co-occurrence matrix in sparse csr format\n",
    "#Yc.todense()\n",
    "sum_occ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
